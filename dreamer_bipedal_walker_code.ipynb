{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Disclosure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is based off of the Samsung Labs implementation: https://github.com/SamsungLabs/tqc_pytorch\n",
    "More information about TQC can be found here: https://arxiv.org/abs/2005.04269\n",
    "\n",
    "An enhancement called D2RL was also implemented following this paper: https://arxiv.org/abs/2010.09163\n",
    "The code associated with this paper: https://github.com/pairlab/d2rl\n",
    "\n",
    "An ERE buffer is implemented to improve sample efficiency, for which the paper can be found here: https://arxiv.org/abs/1906.04009\n",
    "\n",
    "Finally, key ideas and concepts for the auto-regressive transformer design stem from this paper: https://arxiv.org/abs/2202.09481\n",
    "\"\"\"\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from torch.nn import Module, Linear, Transformer, MSELoss, CrossEntropyLoss\n",
    "from torch.distributions import Distribution, Normal\n",
    "from torch.nn.functional import gelu, logsigmoid\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as disp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_TIMESTEPS = 2000\n",
    "\n",
    "# abstracted training loop function for both environments to work\n",
    "# the gym environment will always have MAX_TIMESTEPS as the max_timesteps value\n",
    "# while the dreamer enviroment might have less timesteps\n",
    "def train_on_environement(actor, env, class_to_take_gradient_step, replay_buffer, max_timesteps, state, batch_size, total_num_steps, sequence_length):\n",
    "    episode_timesteps = 0\n",
    "    ep_reward = 0\n",
    "\n",
    "    # save start sequence for the dreamer model\n",
    "    input_buffer = torch.empty((0, 54), device=DEVICE)\n",
    "    for t in range(max_timesteps): \n",
    "        total_num_steps += 1\n",
    "        action = actor.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_timesteps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        # change reward of real environment\n",
    "        # do not store memories if simulation ends without\n",
    "        # reaching the sequence length\n",
    "        if max_timesteps == MAX_TIMESTEPS:\n",
    "            if reward == -100.0:\n",
    "                reward = - 10.0\n",
    "            else:\n",
    "                reward *= 2\n",
    "            replay_buffer.add(state, action, next_state, reward, done)\n",
    "        else:\n",
    "            if t == sequence_length:\n",
    "                for row in input_buffer.cpu().numpy():\n",
    "                    replay_buffer.add(row[:24], row[24:28], row[28:52], row[52], row[53])\n",
    "            elif t > sequence_length:\n",
    "                replay_buffer.add(state, action, next_state, reward, done)\n",
    "        if t < sequence_length:\n",
    "            input_buffer = torch.cat([input_buffer, torch.tensor(np.concatenate((state, action, next_state, np.array([reward]), np.array([done])), axis=0), device=DEVICE).unsqueeze(0)], axis=0)        \n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "        if total_num_steps >= batch_size:\n",
    "            # train the agent using experiences from the real environment\n",
    "            class_to_take_gradient_step.take_gradient_step(replay_buffer, t, batch_size)\n",
    "    \n",
    "        if done:\n",
    "            break\n",
    "    if sequence_length > 0:\n",
    "        return episode_timesteps, ep_reward, input_buffer\n",
    "    return episode_timesteps, ep_reward, None\n",
    "\n",
    "# test loop for agent on environment\n",
    "def simulate_on_environement(actor, env, max_timesteps, state):\n",
    "    episode_timesteps = 0\n",
    "    ep_reward = 0\n",
    "    for t in range(max_timesteps):\n",
    "        action = actor.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_timesteps += 1\n",
    "    \n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "    \n",
    "        if done or t == max_timesteps - 1:\n",
    "            break\n",
    "    return episode_timesteps, ep_reward\n",
    "\n",
    "# generate value array from replay buffer\n",
    "def gen_values_from_replay_buffer(replay_buffer, ptr):\n",
    "    return np.concatenate((\n",
    "                    replay_buffer.state[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.action[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.reward[ptr:replay_buffer.ptr],\n",
    "                    1. - replay_buffer.not_done[ptr:replay_buffer.ptr],\n",
    "                    replay_buffer.next_state[ptr:replay_buffer.ptr],                \n",
    "                ), \n",
    "                axis = 1\n",
    "            )\n",
    "\n",
    "# function to create sequences with a given window size and step size\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    n_memories = values.shape[0]\n",
    "    n_sequences = math.ceil(n_memories / step_size) - math.floor(window_size / step_size) + (1 if n_memories % step_size == 0 and window_size % step_size == 0 else 0)\n",
    "    sequences = np.zeros((n_sequences, window_size, values.shape[1]))\n",
    "    for i in range(n_sequences):\n",
    "        sequences[i, :] = values[i * step_size:i * step_size + window_size, :]\n",
    "    return sequences\n",
    "\n",
    "# function to split train and test data\n",
    "def generate_train_and_test_sequences(replay_buffer, train_set, test_set, train_split, window_size, step_size, ptr):\n",
    "    values = gen_values_from_replay_buffer(replay_buffer, ptr)\n",
    "    try:\n",
    "        memory_sequences = create_sequences(values, window_size, step_size)\n",
    "    except:\n",
    "        return train_set, test_set\n",
    "    indices = np.arange(memory_sequences.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(train_split * memory_sequences.shape[0])\n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "    if train_set is None:\n",
    "        return memory_sequences[train_indices, :], memory_sequences[test_indices, :]\n",
    "    return np.concatenate((train_set, memory_sequences[train_indices, :]), axis=0), np.concatenate((test_set, memory_sequences[test_indices, :]), axis=0)\n",
    "\n",
    "\n",
    "# Calculate the number of training steps for the agent on the dreamer.\n",
    "def calc_dreamer_iterations(dreamer_performance_score, score_threshold, avg_reward):\n",
    "    if dreamer_performance_score >= score_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(10 * (1 - dreamer_performance_score / score_threshold) ** 2)\n",
    "\n",
    "\n",
    "# dreamer agent that will generate\n",
    "class DreamerAgent(Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, seq_len, num_layers, num_heads, dropout_prob, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.input_dim = state_dim + action_dim + 2\n",
    "        self.target_dim = self.state_dim + self.action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.mse_loss = MSELoss()\n",
    "        self.ce_loss = CrossEntropyLoss()\n",
    "        \n",
    "        self.input_fc = Linear(self.input_dim, hidden_dim, device=DEVICE) # \n",
    "        self.target_fc = Linear(self.target_dim, hidden_dim, device=DEVICE) \n",
    "        self.transformer = Transformer(\n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            num_heads, \n",
    "            dropout=dropout_prob,\n",
    "            device=DEVICE,\n",
    "            activation=gelu,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_next_state = Linear(hidden_dim + self.target_dim, state_dim, device=DEVICE)\n",
    "        self.output_reward = Linear(hidden_dim + self.target_dim, 1, device=DEVICE)\n",
    "        self.output_done = Linear(hidden_dim + self.target_dim, 1, device=DEVICE)\n",
    "        self.optimizer = Adam(self.parameters(), lr=3e-4)\n",
    "        \n",
    "    # separate out the ground truth variables and compare against predictions\n",
    "    def loss_fn(self, output_next_state, output_reward, output_done, ground_truth):\n",
    "        reward, done, next_state = torch.split(ground_truth, [1, 1, self.state_dim], dim=-1)\n",
    "        loss = self.mse_loss(output_next_state[:, -1], next_state)\n",
    "        loss += self.mse_loss(output_reward[:, -1], reward)\n",
    "        loss += self.ce_loss(output_done[:, -1], done)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # separate the input and target tensors\n",
    "        target = input_tensor[:, -1, :self.target_dim].unsqueeze(1)\n",
    "        encoded_target = self.target_fc(target)\n",
    "        encoded_input = self.input_fc(input_tensor[:, :-1, :self.input_dim])\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target], axis=2))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target], axis=2))\n",
    "        output_done = sigmoid(self.output_done(torch.cat([encoded_output, target], axis=2)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "    \n",
    "    def predict(self, input_tensor, target_tensor):\n",
    "        # separate the input and target tensors\n",
    "        encoded_target = self.target_fc(target_tensor)\n",
    "        encoded_input = self.input_fc(input_tensor)\n",
    "\n",
    "        # pass these into the transformer\n",
    "        encoded_output = self.transformer(encoded_input, encoded_target)\n",
    "\n",
    "        # decode the densely connected output\n",
    "        output_next_state = self.output_next_state(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_reward = self.output_reward(torch.cat([encoded_output, target_tensor], axis=1))\n",
    "        output_done = sigmoid(self.output_done(torch.cat([encoded_output, target_tensor], axis=1)))\n",
    "        return output_next_state, output_reward, output_done\n",
    "\n",
    "    # transformer training loop\n",
    "    # sequences shape: (batch, sequence, features)\n",
    "    def train_dreamer(self, sequences, epochs, batch_size=256):\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float, device=DEVICE)\n",
    "        \n",
    "        train_dataset = TensorDataset(inputs)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(train_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                self.optimizer.zero_grad()\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, running_loss / len(train_dataloader)))\n",
    "\n",
    "    # transformer testing loop\n",
    "    def test_dreamer(self, sequences, batch_size=64):\n",
    "        inputs = torch.tensor(sequences, dtype=torch.float, device=DEVICE)\n",
    "        \n",
    "        test_dataset = TensorDataset(inputs)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for i, input_batch in enumerate(test_dataloader):\n",
    "                input_batch = input_batch[0]\n",
    "                output_next_state, output_reward, output_done = self.forward(input_batch)\n",
    "                loss = self.loss_fn(output_next_state, output_reward, output_done, input_batch[:, -1, self.target_dim:])\n",
    "                running_loss += loss.item()\n",
    "            print('Test Loss: {:.4f}'.format(running_loss / len(test_dataloader)))\n",
    "        return running_loss / len(test_dataloader)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.actions = torch.cat([self.actions, torch.tensor(np.array([action]), device=DEVICE)], axis=0)\n",
    "        input_sequence = torch.cat([self.states[:-1], self.actions[:-1], self.rewards, self.dones], axis=1).to(torch.float32)\n",
    "        target = torch.cat([self.states[-1], self.actions[-1]], axis=0).unsqueeze(0).to(torch.float32)\n",
    "        with torch.no_grad():\n",
    "            next_state, reward, done = self.predict(input_sequence, target)\n",
    "            done = (done >= 0.6).float() # bias towards not done to avoid false positives\n",
    "\n",
    "            self.states = torch.cat([self.states, next_state], axis=0)\n",
    "            self.rewards = torch.cat([self.rewards, reward], axis=0)\n",
    "            self.dones = torch.cat([self.dones, done], axis=0)\n",
    "            \n",
    "            if self.states.shape[0] > self.seq_len:\n",
    "                self.states = self.states[1:]\n",
    "                self.rewards = self.rewards[1:]\n",
    "                self.dones = self.dones[1:]\n",
    "            if self.actions.shape[0] > self.seq_len - 1:\n",
    "                self.actions = self.actions[1:]\n",
    "        \n",
    "        return next_state.squeeze(0).cpu().numpy(), reward.cpu().item(), done.cpu().item(), None\n",
    "\n",
    "\n",
    "def quantile_huber_loss(quantiles, samples, sum_over_quantiles = False):\n",
    "    #return huber loss - uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise\n",
    "    delta = samples[:, np.newaxis, np.newaxis, :] - quantiles[:, :, :, np.newaxis]  \n",
    "    abs_delta = torch.abs(delta)\n",
    "    huber_loss = torch.where(abs_delta > 1, abs_delta - 0.5, delta ** 2 * 0.5)\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    cumulative_prob = (torch.arange(n_quantiles, device=quantiles.device, dtype=torch.float) + 0.5) / n_quantiles\n",
    "    cumulative_prob_shaped = cumulative_prob.view(1, 1, -1, 1)\n",
    "    loss = (torch.abs(cumulative_prob_shaped - (delta < 0).float()) * huber_loss)\n",
    "\n",
    "    # Summing over the quantile dimension \n",
    "    if sum_over_quantiles:\n",
    "        loss = loss.sum(dim=-2).mean()\n",
    "    else:\n",
    "        loss = loss.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#MLP for critic that implements D2RL architecture \n",
    "class Mlp_for_Critic(Module):\n",
    "    def __init__(self,input_size,hidden_sizes,output_size):\n",
    "        super().__init__()\n",
    "        input_size_ = input_size\n",
    "        input_dim = 28 + hidden_sizes[0] \n",
    "        self.list_of_layers = []\n",
    "        for i, next_size in enumerate(hidden_sizes):\n",
    "            if i == 0:\n",
    "              lay = Linear(input_size_, next_size, device=DEVICE)\n",
    "            else: \n",
    "              lay = Linear(input_dim, next_size, device=DEVICE)\n",
    "            self.add_module(f'layer{i}', lay)\n",
    "            self.list_of_layers.append(lay)\n",
    "        self.last_layer = Linear(input_dim, output_size, device=DEVICE)\n",
    "    \n",
    "\n",
    "    def forward(self, input_):\n",
    "        curr = input_\n",
    "        for lay in self.list_of_layers:\n",
    "            curr_ = gelu(lay(curr))\n",
    "            curr = torch.cat([curr_, input_], dim = 1)\n",
    "        output = self.last_layer(curr)\n",
    "        return output\n",
    "\n",
    "\n",
    "#MLP for actor that implements D2RL architecture\n",
    "class Mlp_for_Actor(Module):\n",
    "    def __init__(self,input_size,hidden_sizes,output_size):\n",
    "        super().__init__()\n",
    "        self.list_of_layers = []\n",
    "        input_size_ = input_size\n",
    "        num_inputs = 24 \n",
    "        input_dim = hidden_sizes[0] + num_inputs\n",
    "        for i, next_size in enumerate(hidden_sizes):\n",
    "            if i == 0:\n",
    "              lay = Linear(input_size_, next_size, device=DEVICE)\n",
    "            else:\n",
    "              lay = Linear(input_dim, next_size, device=DEVICE)\n",
    "            self.add_module(f'layer{i}', lay)\n",
    "            self.list_of_layers.append(lay)\n",
    "            input_size_ = next_size\n",
    "            \n",
    "        self.last_layer_mean_linear = Linear(input_dim, output_size, device=DEVICE)\n",
    "        self.last_layer_log_std_linear = Linear(input_dim, output_size, device=DEVICE)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        curr = input_\n",
    "\n",
    "        for layer in self.list_of_layers:\n",
    "            intermediate = layer(curr)\n",
    "            curr = gelu(intermediate)\n",
    "\n",
    "            curr = torch.cat([curr, input_], dim=1)\n",
    "\n",
    "        mean_linear = self.last_layer_mean_linear(curr)\n",
    "        log_std_linear = self.last_layer_log_std_linear(curr)\n",
    "        return mean_linear, log_std_linear\n",
    "\n",
    "\n",
    "class EREReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, T, max_size=int(1e6), eta=0.996, cmin=5000):\n",
    "        self.max_size, self.ptr, self.size, self.rollover = max_size, 0, 0, False\n",
    "        self.eta0 = eta\n",
    "        self.cmin = cmin\n",
    "        self.c_list = []\n",
    "        self.index = []\n",
    "        self.T = T\n",
    "\n",
    "        self.reward = np.empty((max_size, 1))\n",
    "        self.state = np.empty((max_size, state_dim))\n",
    "        self.action = np.empty((max_size, action_dim))\n",
    "        self.not_done = np.empty((max_size, 1))\n",
    "        self.next_state = np.empty((max_size, state_dim))\n",
    "        \n",
    "    def sample(self, batch_size, t):\n",
    "        \n",
    "        # eta value for current timestep\n",
    "        eta = self.eta_anneal(t)\n",
    "\n",
    "        index = np.array([self._get_index(eta, k, batch_size) for k in range(batch_size)])\n",
    "\n",
    "        r = torch.tensor(self.reward[index], dtype = torch.float, device = DEVICE)\n",
    "        s = torch.tensor(self.state[index], dtype = torch.float, device = DEVICE)\n",
    "        ns = torch.tensor(self.next_state[index], dtype = torch.float, device = DEVICE)\n",
    "        a = torch.tensor(self.action[index], dtype = torch.float, device = DEVICE)\n",
    "        nd = torch.tensor(self.not_done[index], dtype = torch.float, device = DEVICE)\n",
    "        \n",
    "        return s, a, ns, r, nd\n",
    "    \n",
    "    def _get_index(self, eta, k, batch_size):\n",
    "        c_calc = self.size * eta ** (k * 1000 / batch_size)\n",
    "        ck = c_calc if c_calc > self.cmin else self.size\n",
    "\n",
    "        if not self.rollover:\n",
    "            return np.random.randint(self.size - ck, self.size)\n",
    "        \n",
    "        return np.random.randint(self.ptr + self.size - ck, self.ptr + self.size) % self.size\n",
    "\n",
    "    def eta_anneal(self, t):\n",
    "       return self.eta0 + (1 - self.eta0) * t / self.T\n",
    " \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        #Add experience to replay buffer \n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.ptr %= self.max_size\n",
    "\n",
    "        if self.max_size > self.size + 1:\n",
    "          self.size = self.size + 1\n",
    "        else:\n",
    "          self.size = self.max_size\n",
    "          self.rollover = True\n",
    "\n",
    "\n",
    "#Actor\n",
    "class Actor(Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = Mlp_for_Actor(state_dim, [512, 512], action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.mlp(obs)\n",
    "        log_std = log_std.clamp(-20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        log_prob = None\n",
    "        if self.training == False: \n",
    "            action = torch.tanh(mean)\n",
    "        elif self.training == True:\n",
    "            tanh_dist = TanhNormal(mean, std)\n",
    "            action, pre_tanh = tanh_dist.random_sample()\n",
    "            log_prob = tanh_dist.log_probability(pre_tanh)\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True)      \n",
    "        else:  \n",
    "            print('Something wrong with training mode')\n",
    "            \n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs).to(DEVICE)[np.newaxis, :]\n",
    "        action, log_prob = self.forward(obs)\n",
    "        return np.array(action[0].cpu().detach())\n",
    "\n",
    "\n",
    "#Critic\n",
    "class Critic(Module):\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets):\n",
    "        super().__init__()\n",
    "        self.list_of_mlp = []\n",
    "        self.n_quantiles = n_quantiles\n",
    "\n",
    "        for i in range(n_nets):\n",
    "            net = Mlp_for_Critic(state_dim + action_dim, [256, 256], n_quantiles)\n",
    "            self.add_module(f'net{i}', net)\n",
    "            self.list_of_mlp.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        quantiles = torch.stack(tuple(net(torch.cat((state, action), dim=1)) for net in self.list_of_mlp), dim=1)\n",
    "        return quantiles\n",
    "\n",
    "\n",
    "\n",
    "class TanhNormal(Distribution):\n",
    "    def __init__(self, normal_mean, normal_std):\n",
    "        super().__init__()\n",
    "        self.normal_mean = normal_mean\n",
    "        self.normal_std = normal_std\n",
    "        self.normal = Normal(normal_mean, normal_std)\n",
    "        self.stand_normal = Normal(torch.zeros_like(self.normal_mean, device=DEVICE), torch.ones_like(self.normal_std, device=DEVICE))\n",
    "        \n",
    "        \n",
    "    def logsigmoid(tensor):\n",
    "\n",
    "      denominator = 1 + torch.exp(-tensor)\n",
    "      return torch.log(1/ denominator)\n",
    "\n",
    "    def log_probability(self, pre_tanh):\n",
    "        final = (self.normal.log_prob(pre_tanh)) - (2 * np.log(2) + logsigmoid(2 * pre_tanh) + logsigmoid(-2 * pre_tanh))\n",
    "        return final\n",
    "\n",
    "    def random_sample(self):\n",
    "        pretanh = self.normal_mean + self.normal_std * self.stand_normal.sample()\n",
    "        return torch.tanh(pretanh), pretanh\n",
    "\n",
    "\n",
    "class Gradient_Step(object):\n",
    "  def __init__(\n",
    "    self,\n",
    "    *,\n",
    "    actor,\n",
    "    critic,\n",
    "    critic_target,\n",
    "    discount,\n",
    "    tau,\n",
    "    top_quantiles_to_drop,\n",
    "    target_entropy,\n",
    "    quantiles_total\n",
    "  ):\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "    self.critic_target = critic_target\n",
    "    self.log_alpha = torch.zeros((1,), requires_grad=True, device=DEVICE)\n",
    "    self.quantiles_total = quantiles_total\n",
    "    self.actor_optimizer = Adam(self.actor.parameters(), lr=3e-4)\n",
    "    \n",
    "    self.alpha_optimizer = Adam([self.log_alpha], lr=3e-4)\n",
    "    self.critic_optimizer = Adam(self.critic.parameters(), lr=3e-4)\n",
    "    self.discount, self.tau, self.top_quantiles_to_drop, self.target_entropy  = discount, tau, top_quantiles_to_drop,target_entropy\n",
    "\n",
    "\n",
    "  def take_gradient_step(self, replay_buffer, t, batch_size=256):\n",
    "    # Sample replay buffer\n",
    "    state, action, next_state, reward, not_done = replay_buffer.sample(batch_size, t)\n",
    "    alpha = torch.exp(self.log_alpha) #entropy temperature coefficient\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # Action by the current actor for the sampled state\n",
    "      new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "      # Compute and cut quantiles at the next state\n",
    "      next_z = self.critic_target(next_state, new_next_action)  \n",
    "      \n",
    "      # Sort and drop top k quantiles to control overestimation.\n",
    "      sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "      sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop]\n",
    "\n",
    "      # td error + entropy term\n",
    "      target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "    \n",
    "    # Get current Quantile estimates using action from the replay buffer\n",
    "    cur_z = self.critic(state, action)\n",
    "    critic_loss = quantile_huber_loss(cur_z, target)\n",
    "\n",
    "\n",
    "    new_action, log_pi = self.actor(state)\n",
    "    # detach the variable from the graph so we don't change it with other losses\n",
    "    alpha_loss = -self.log_alpha * (log_pi + self.target_entropy).detach().mean()\n",
    "\n",
    "    # Optimise critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # Update target networks\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Compute actor loss\n",
    "    actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "    \n",
    "    # Optimise the actor\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # Optimise the entropy coefficient\n",
    "    self.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    self.alpha_optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interval = 10 # update the plot every N episodes\n",
    "video_every = 25 # videos can take a very long time to render so only do it every N episodes\n",
    "\n",
    "# agent hyperparameters\n",
    "seed = 42\n",
    "n_quantiles = 25\n",
    "top_quantiles_to_drop_per_net = 2\n",
    "n_nets = 5\n",
    "batch_size = 256\n",
    "discount = 0.98\n",
    "tau = 0.005\n",
    "\n",
    "# dreamer hyperparameters\n",
    "batch_size_dreamer = 512\n",
    "hidden_dim = 256\n",
    "num_layers = 16\n",
    "num_heads = 4\n",
    "dropout_prob = 0.1\n",
    "window_size = 40               # transformer context window size\n",
    "step_size = 1                  # how many timesteps to skip between each context window\n",
    "train_split = 0.80             # train/validation split\n",
    "score_threshold = 0.8          # quality threshold for using the dreamer model\n",
    "dreamer_train_epochs = 15      # how many epochs to train the dreamer model for\n",
    "dreamer_train_frequency = 10   # how often to train the dreamer model\n",
    "episode_threshold = 50         # how many episodes to run before training the dreamer model\n",
    "max_size = int(5e4)            # maximum size of the training set for the dreamer model\n",
    "\n",
    "record = True\n",
    "save_model = True\n",
    "\n",
    "env = gym.make('BipedalWalker-v3')\n",
    "env.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if record:\n",
    "    env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id % video_every == 0 and ep_id >= 50, force=True)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# max_episodes = 100\n",
    "max_timesteps = MAX_TIMESTEPS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialisation and Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialise everything\n",
    "replay_buffer = EREReplayBuffer(state_dim, action_dim, MAX_TIMESTEPS)\n",
    "actor = Actor(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "critic = Critic(state_dim, action_dim, n_quantiles, n_nets).to(DEVICE)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "dreamer = DreamerAgent(state_dim, action_dim, hidden_dim, window_size, num_layers, num_heads, dropout_prob).to(DEVICE)\n",
    "\n",
    "top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_nets\n",
    "\n",
    "class_to_take_gradient_step = Gradient_Step(actor=actor,critic=critic,critic_target=critic_target,top_quantiles_to_drop=top_quantiles_to_drop,discount=discount,tau=tau,target_entropy=-np.prod(env.action_space.shape).item(), quantiles_total = n_quantiles * n_nets)\n",
    "actor.train()\n",
    "state = env.reset()\n",
    "\n",
    "episode_timesteps = 0\n",
    "episode = 1\n",
    "total_num_steps = 0\n",
    "ep_reward = 0\n",
    "memory_ptr = 0\n",
    "train_set, test_set = None, None\n",
    "reward_list = []\n",
    "reward_avg_list = []\n",
    "plot_data = []\n",
    "episode_timesteps_dreamer = ep_reward_dreamer = dreamer_iterations = 0\n",
    "current_size = 0\n",
    "\n",
    "log_f = open(\"agent-log.txt\",\"w+\")\n",
    "episode = 1\n",
    "while True:\n",
    "\n",
    "    # sample from true environment\n",
    "    episode_timesteps, ep_reward, input_buffer = train_on_environement(actor, env, class_to_take_gradient_step, replay_buffer, max_timesteps, state, batch_size, total_num_steps, window_size)\n",
    "    total_num_steps += episode_timesteps\n",
    "    state = env.reset()\n",
    "\n",
    "    # generate and trim the size of the train/test sets\n",
    "    train_set, test_set = generate_train_and_test_sequences(replay_buffer, train_set, test_set, train_split, window_size, step_size, memory_ptr)\n",
    "\n",
    "    if episode >= episode_threshold and input_buffer.shape[0] == window_size:  \n",
    "\n",
    "        # train and assess the dreamer every train_frequency\n",
    "        if episode % dreamer_train_frequency == 0:\n",
    "            dreamer.train_dreamer(train_set, dreamer_train_epochs, batch_size_dreamer)\n",
    "\n",
    "        # truncate the training set to control train time performance\n",
    "        if test_set.shape[0] > max_size:\n",
    "            train_set = train_set[-max_size:]\n",
    "        \n",
    "        # Evaluate the dreamer's performance\n",
    "        dreamer_effectiveness_score = dreamer.test_dreamer(test_set, batch_size_dreamer)\n",
    "        dreamer_iterations = calc_dreamer_iterations(dreamer_effectiveness_score, score_threshold, np.array(reward_avg_list).mean())        \n",
    "\n",
    "        print('Size of sequences: ', train_set.shape[0], test_set.shape[0])        \n",
    "\n",
    "        if dreamer_iterations > 0:\n",
    "            print(f'Dreamer active for {dreamer_iterations} iterations')\n",
    "            episode_timesteps_dreamer = ep_reward_dreamer = 0\n",
    "            # train the agent on the dreamer if the dreamer is good enough to accurately depict the environment\n",
    "            for dep in range(dreamer_iterations):\n",
    "                print('Dreamer episode: ', dep+1)\n",
    "\n",
    "                # initialise dreamer states with the input sequence\n",
    "                dreamer.states = input_buffer[:, :state_dim]\n",
    "                dreamer.actions = input_buffer[:-1, state_dim:state_dim+action_dim]\n",
    "                dreamer.rewards = input_buffer[:-1, -2-state_dim].unsqueeze(1)\n",
    "                dreamer.dones = input_buffer[:-1, -1-state_dim].unsqueeze(1)\n",
    "\n",
    "            # sample from dreamer environment\n",
    "            _td, _rd, _ = train_on_environement(actor, dreamer, class_to_take_gradient_step, replay_buffer, math.ceil(total_num_steps/episode) - 1, dreamer.states[-1].cpu().numpy(), batch_size, total_num_steps, window_size)\n",
    "            episode_timesteps_dreamer += _td\n",
    "            ep_reward_dreamer += _rd\n",
    "    memory_ptr = replay_buffer.ptr\n",
    "\n",
    "    # save results and reset variables\n",
    "    # NOTE dreamer rewards are based on values stored in the replay buffer,\n",
    "    # which are modified to have fall penalty -100 -> -10 and all other rewards\n",
    "    # scaled by a factor of 2\n",
    "    print(f\"Episode Num: {episode} Episode T: {episode_timesteps} Reward: {ep_reward:.3f} Dreamer Eps: {dreamer_iterations} Dreamer Avg Timesteps: {episode_timesteps_dreamer/dreamer_iterations if dreamer_iterations > 0 else 0} Dreamer Avg Reward: {ep_reward_dreamer/dreamer_iterations if dreamer_iterations > 0 else 0}\")\n",
    "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
    "    log_f.flush()\n",
    "    reward_list.append(ep_reward)\n",
    "    reward_avg_list.append(ep_reward)\n",
    "    ep_reward = 0\n",
    "    episode += 1\n",
    "\n",
    "    # print reward data every so often\n",
    "    if episode % plot_interval == 0:\n",
    "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
    "        reward_list = []\n",
    "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
    "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "        plt.xlabel('Episode number')\n",
    "        plt.ylabel('Episode reward')\n",
    "        plt.show()\n",
    "        disp.clear_output(wait=True)\n",
    "\n",
    "    # break condition\n",
    "    if len(reward_avg_list) == 100:\n",
    "        print(f'Current progress: {np.array(reward_avg_list).mean()}/300')\n",
    "        if np.array(reward_avg_list).mean() >= 300:\n",
    "            print('Completed environment!')\n",
    "            break\n",
    "        if episode == 1000:\n",
    "            print('Exceeded training time')\n",
    "            break\n",
    "        reward_avg_list = reward_avg_list[-99:]\n",
    "\n",
    "if save_model:\n",
    "    torch.save(actor.state_dict(), './model.pth')\n",
    "env.close()\n",
    "\n",
    "with open(\"plot.txt\", \"w\") as file:\n",
    "    for episode, mean, std in plot_data:\n",
    "        file.write(str(episode) + ',' + str(mean) + ',' + str(std) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13ad313fd030a9371d549742a2f00e4a3642d0405ea4341c671e6fcbd57a5cbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
